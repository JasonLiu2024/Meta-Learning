{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ca00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import numpy as np\n",
    "# path dictionary\n",
    "path_data_folder = \"5.9.23/\"\n",
    "path_dictionary = {\n",
    "    '20.7': path_data_folder + \"twentypointseven\",\n",
    "    '21.0': path_data_folder + \"twentyonepointzero_1\",\n",
    "    '21.2': path_data_folder + \"twentyonepointtwo_1\",\n",
    "    '21.5': path_data_folder + \"twentyonepointfive_1\",\n",
    "    '21.7': path_data_folder + \"TwentyonepointseevendegreeC_1\",\n",
    "    '21.8': path_data_folder + \"twentypointeight_1\"\n",
    "}\n",
    "# separator in this file is tab\n",
    "# label for entire file is the temperature\n",
    "# frames = pd.read_csv(\"5.9.23/twentypointseven\", sep=\"\\t\", header=None)\n",
    "# # <- pandas index is [column][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84655847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data are (1000 * 1) column vectors.\n",
    "# in the file, there are 1000 lines, each with n numbers, \n",
    "# where n = number of data vectors\n",
    "def load_data(filename_dictionary):\n",
    "    X_data = [] # data\n",
    "    y_data = [] # label\n",
    "    for filename, filepath in filename_dictionary.items():\n",
    "        print(f\"reading file:        {filepath}\")\n",
    "        X_in_this_file = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        value = float(filename)\n",
    "        print(f\"\\ttemperature value: {value}\")\n",
    "        number_of_examples = X_in_this_file.shape[1]\n",
    "        y_in_this_file = np.zeros(shape=(1, number_of_examples)) + value\n",
    "        y_in_this_file = pd.DataFrame(y_in_this_file)\n",
    "        # default column setting is NO array, \n",
    "        # need to make it array to use list of indices!\n",
    "        y_in_this_file.columns = np.asarray(range(y_in_this_file.shape[1]))\n",
    "        X_data.append(X_in_this_file)\n",
    "        y_data.append(y_in_this_file)\n",
    "    X_data = pd.concat(X_data, axis=1, ignore_index=True)\n",
    "    y_data = pd.concat(y_data, axis=1, ignore_index=True)\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0373a910-6be0-4875-aae2-f58929440230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file:        5.9.23/twentypointseven\n",
      "\ttemperature value: 20.7\n",
      "reading file:        5.9.23/twentyonepointzero_1\n",
      "\ttemperature value: 21.0\n",
      "reading file:        5.9.23/twentyonepointtwo_1\n",
      "\ttemperature value: 21.2\n",
      "reading file:        5.9.23/twentyonepointfive_1\n",
      "\ttemperature value: 21.5\n",
      "reading file:        5.9.23/TwentyonepointseevendegreeC_1\n",
      "\ttemperature value: 21.7\n",
      "reading file:        5.9.23/twentypointeight_1\n",
      "\ttemperature value: 21.8\n",
      "total number of examples:     60000\n",
      "length of each example:       1000\n",
      "shape of X data (spectrum): (1000, 60000), type: float64\n",
      "shape of y data (temperature): (1, 60000), type: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import pandas as pd\n",
    "# change: response (X) -> spectrum, spectra (y) -> temperature\n",
    "\n",
    "spectrum_raw, temperature_raw = load_data(filename_dictionary=path_dictionary)\n",
    "print(f\"total number of examples:     {spectrum_raw.shape[1]}\")\n",
    "print(f\"length of each example:       {spectrum_raw.shape[0]}\")\n",
    "print(f\"shape of X data (spectrum): {spectrum_raw.shape}, type: {spectrum_raw[0][0].dtype}\")\n",
    "print(f\"shape of y data (temperature): {temperature_raw.shape}, type: {temperature_raw[0].dtype}\")\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723e2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with 66th row\n",
      "normalized mean will not be exactly 0, but very close to it!\n",
      "SPECTRUM\n",
      "0th row: before normalization: 0        0.816\n",
      "1        0.832\n",
      "2        0.856\n",
      "3        0.872\n",
      "4        0.864\n",
      "         ...  \n",
      "59995    0.888\n",
      "59996    0.896\n",
      "59997    0.888\n",
      "59998    0.912\n",
      "59999    0.912\n",
      "Name: 66, Length: 60000, dtype: float64\n",
      "mean = 0.9231629333332636\n",
      "0th row: after normalization: 0       -1.105015\n",
      "1       -0.940031\n",
      "2       -0.692553\n",
      "3       -0.527569\n",
      "4       -0.610061\n",
      "           ...   \n",
      "59995   -0.362584\n",
      "59996   -0.280092\n",
      "59997   -0.362584\n",
      "59998   -0.115107\n",
      "59999   -0.115107\n",
      "Name: 66, Length: 60000, dtype: float64\n",
      "mean = -5.432357387998484e-14\n",
      "TEMPERATURE\n",
      "0th row: before normalization: 0        20.7\n",
      "1        20.7\n",
      "2        20.7\n",
      "3        20.7\n",
      "4        20.7\n",
      "         ... \n",
      "59995    21.8\n",
      "59996    21.8\n",
      "59997    21.8\n",
      "59998    21.8\n",
      "59999    21.8\n",
      "Name: 0, Length: 60000, dtype: float64\n",
      "mean = 21.31666666666431\n",
      "0th row: after normalization: 0       -1.584906\n",
      "1       -1.584906\n",
      "2       -1.584906\n",
      "3       -1.584906\n",
      "4       -1.584906\n",
      "           ...   \n",
      "59995    1.242224\n",
      "59996    1.242224\n",
      "59997    1.242224\n",
      "59998    1.242224\n",
      "59999    1.242224\n",
      "Name: 0, Length: 60000, dtype: float64\n",
      "mean = -4.1222627533699095e-13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"normalization: longer sklearn implementation\"\"\"\n",
    "# more code, but can change type of scaler easily\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "scaler = StandardScaler() # change scaler here\n",
    "index = random.randint(0, spectrum_raw.shape[0] - 1)\n",
    "print(f\"test with {index}th row\")\n",
    "print(\"normalized mean will not be exactly 0, but very close to it!\")\n",
    "spectrum = spectrum_raw.copy()\n",
    "for i in range(spectrum_raw.shape[0]):\n",
    "    # StandardScaler only takes 2d array\n",
    "    array = np.reshape(np.asarray(spectrum_raw.iloc[i]), (-1, 1))\n",
    "    transformed = scaler.fit_transform(array)\n",
    "    spectrum.iloc[i] = np.reshape(transformed, -1)\n",
    "print(\"SPECTRUM\")\n",
    "print(f\"0th row: before normalization: {spectrum_raw.iloc[index]}\")\n",
    "print(f\"mean = {sum(spectrum_raw.iloc[index])/spectrum_raw.shape[1]}\")\n",
    "print(f\"0th row: after normalization: {spectrum.iloc[index]}\")\n",
    "print(f\"mean = {sum(spectrum.iloc[index])/spectrum_raw.shape[1]}\")\n",
    "temperature = temperature_raw.copy()\n",
    "for i in range(temperature_raw.shape[0]):\n",
    "    # StandardScaler only takes 2d array\n",
    "    array = np.reshape(np.asarray(temperature_raw.iloc[i]), (-1, 1))\n",
    "    transformed = scaler.fit_transform(array)\n",
    "    temperature.iloc[i] = np.reshape(transformed, -1)\n",
    "print(\"TEMPERATURE\")\n",
    "print(f\"0th row: before normalization: {temperature_raw.iloc[0]}\")\n",
    "print(f\"mean = {sum(temperature_raw.iloc[0])/spectrum_raw.shape[1]}\")\n",
    "print(f\"0th row: after normalization: {temperature.iloc[0]}\")\n",
    "print(f\"mean = {sum(temperature.iloc[0])/spectrum_raw.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8584e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorter pandas implementation, also work (very small difference in value)\n",
    "# less code, but more changes needed to change normalization method\n",
    "# spectrum = spectrum_raw.sub(spectrum_raw.mean(axis=1), axis=0).div(spectrum_raw.std(axis=1), axis=0)\n",
    "# print(f\"0th row: before normalization: {spectrum_raw.iloc[0]}\")\n",
    "# print(f\"mean = {sum(spectrum_raw.iloc[0])/spectrum_raw.shape[1]}\")\n",
    "# print(f\"0th row: after normalization: {spectrum.iloc[0]}\")\n",
    "# print(f\"mean = {sum(spectrum.iloc[0])/spectrum_raw.shape[1]}\")\n",
    "# temperature = temperature_raw.sub(temperature_raw.mean(axis=1), axis=0).div(temperature_raw.std(axis=1), axis=0)\n",
    "# print(f\"0th row: before normalization: {temperature_raw.iloc[0]}\")\n",
    "# print(f\"mean = {sum(temperature_raw.iloc[0])/spectrum_raw.shape[1]}\")\n",
    "# print(f\"0th row: after normalization: {temperature.iloc[0]}\")\n",
    "# print(f\"mean = {sum(temperature.iloc[0])/spectrum_raw.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6558b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got indices from cross_validation_resample=2_fold=5_dnn.pickle\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "file_name = 'cross_validation_resample=2_fold=5_dnn'\n",
    "if os.path.isfile(file_name+'.pickle'): \n",
    "    with open(file_name+'.pickle', 'rb') as handle:\n",
    "        train_indices,test_indices = pickle.load(handle)\n",
    "    print(f\"got indices from {file_name}.pickle\")  \n",
    "else:\n",
    "    # 2x5 resampling\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    number_resamples = 2\n",
    "    n_splits =5\n",
    "    for i in range(number_resamples):\n",
    "        kf = KFold(n_splits=n_splits, random_state=i, shuffle=True)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(kf.split(range(temperature.shape[1]))):\n",
    "            train_indices.append(train_index)\n",
    "            test_indices.append(test_index)\n",
    "    print(f\"got indices by KFold method, fold = {n_splits}, resample = {number_resamples}\")\n",
    "    with open(file_name+'.pickle', 'wb') as handle:\n",
    "        pickle.dump([train_indices,test_indices], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"saved indices in {file_name}.pickle file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55c7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sets of training indices: 10\n",
      "number of training indices per set: 48000\n",
      "sets of testing indices: 10\n",
      "number of testing indices per set: 12000\n"
     ]
    }
   ],
   "source": [
    "print(f\"sets of training indices: {len(train_indices)}\")\n",
    "print(f\"number of training indices per set: {len(train_indices[0])}\")\n",
    "print(f\"sets of testing indices: {len(test_indices)}\")\n",
    "print(f\"number of testing indices per set: {len(test_indices[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e11a5496-4ebb-4ea9-a759-26481d1fb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# change: input dim = 1000, output dim = 1 (temperature value)\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,device, input_dim=1000):\n",
    "        super().__init__()\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.hidden_dim = 500\n",
    "        self.linear1 = torch.nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.linear2= torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.linear3= torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "    def forward(self, x):\n",
    "        y = self.linear3(self.relu(self.linear2(self.relu(self.linear1(x)))))\n",
    "        # change: remove sigmoid, use y result as is\n",
    "        # y = torch.sigmoid(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17847f0f-c9ae-49a7-a84d-e6f82f5cef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class CalculateMSE():\n",
    "    def __init__(self, net, n_epochs, batch_size):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        #initialize some constants\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-4\n",
    "        self.n_epochs = n_epochs\n",
    "        self.net.apply(self.weights_init)   \n",
    "    def weights_init(self,layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.orthogonal_(layer.weight)\n",
    "    def get_mse(self,train_data, train_label, test_data, test_label):\n",
    "        train_set = torch.utils.data.TensorDataset(\n",
    "            torch.Tensor(train_data).to(device), \n",
    "            torch.Tensor(train_label).to(device))\n",
    "        val_set = torch.utils.data.TensorDataset(\n",
    "            torch.Tensor(test_data).to(device), \n",
    "            torch.Tensor(test_label).to(device))\n",
    "        loader_args = dict(batch_size=self.batch_size)\n",
    "        train_loader = DataLoader(train_set, shuffle=True, drop_last=True, **loader_args)\n",
    "        val_loader = DataLoader(val_set, shuffle=True, drop_last=True, **loader_args)\n",
    "        tloss = []\n",
    "        vloss = []\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=self.learning_rate) # weight_decay=0\n",
    "        for epoch in range(0, self.n_epochs):\n",
    "            # if epoch % 1000 == 0:\n",
    "            #     print(f\"epoch = {epoch}\")\n",
    "            epoch_train_loss=[]\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                inputs, label = data\n",
    "                y_pred = self.net(inputs.to(self.net.device))\n",
    "                loss = criterion(y_pred, label.to(self.net.device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss.append(loss.item())\n",
    "            tloss.append(np.mean(epoch_train_loss))\n",
    "            epoch_loss=[]\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                with torch.no_grad():\n",
    "                    inputs1, label1 = data\n",
    "                    y_pred1 = self.net(inputs1.to(self.net.device))\n",
    "                    loss1 = criterion(y_pred1, label1.to(self.net.device))\n",
    "                    epoch_loss.append(loss1.item())\n",
    "            vloss.append(np.mean(epoch_loss))\n",
    "        return np.min(vloss), self.net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6361355-4b60-4910-b158-c54bd4016da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs: 1, batch size: 32, device: cuda\n",
      "we are on fold no.0\n",
      "\tloss: 0.33833638179302217\n",
      "\ttime: 2023-05-18 09:01:30\n",
      "we are on fold no.1\n",
      "\tloss: 0.2672790770928065\n",
      "\ttime: 2023-05-18 09:01:39\n",
      "we are on fold no.2\n",
      "\tloss: 0.3473200652996699\n",
      "\ttime: 2023-05-18 09:01:48\n",
      "we are on fold no.3\n",
      "\tloss: 0.24806785235802334\n",
      "\ttime: 2023-05-18 09:01:57\n",
      "we are on fold no.4\n",
      "\tloss: 0.22492081566651662\n",
      "\ttime: 2023-05-18 09:02:07\n",
      "we are on fold no.5\n",
      "\tloss: 0.22177571150660516\n",
      "\ttime: 2023-05-18 09:02:16\n",
      "we are on fold no.6\n",
      "\tloss: 0.1869705994526545\n",
      "\ttime: 2023-05-18 09:02:25\n",
      "we are on fold no.7\n",
      "\tloss: 0.2209548215866089\n",
      "\ttime: 2023-05-18 09:02:34\n",
      "we are on fold no.8\n",
      "\tloss: 0.16448056971033415\n",
      "\ttime: 2023-05-18 09:02:43\n",
      "we are on fold no.9\n",
      "\tloss: 0.18232015985250474\n",
      "\ttime: 2023-05-18 09:02:52\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "# change: turn into 10 right now for development, was 3000\n",
    "n_epochs=100\n",
    "batch_size=32\n",
    "\n",
    "PATH = 'model_dnn/'\n",
    "Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# change: commented out: alraedy read in X, y data earlier\n",
    "# response = pd.read_csv(\"1127_final_data/response.csv\", header=None).values #input X\n",
    "# spectra = pd.read_csv(\"1127_final_data/spectra.csv\", header=None).values #ground truth label Y\n",
    "# change: input dim = 1000\n",
    "mdl = Model(device=device, input_dim=1000)\n",
    "losses = []\n",
    "print(f\"number of epochs: {n_epochs}, batch size: {batch_size}, device: {mdl.device}\")\n",
    "\n",
    "f = open(\"log_dnn.txt\", \"w\")\n",
    "f.write(\"train START: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\")\n",
    "f.write(f\"number of epochs: {n_epochs}, batch size: {batch_size}, device: {device}\\n\")\n",
    "for i,(train,test) in enumerate(zip(train_indices,test_indices)):\n",
    "    print(f\"we are on fold no.{i}\")\n",
    "    train_data, train_label= spectrum[train],temperature[train]\n",
    "    test_data, test_label= spectrum[test],temperature[test]\n",
    "    mse_calculator = CalculateMSE(mdl,n_epochs,batch_size)\n",
    "    loss,model = mse_calculator.get_mse((np.asarray(train_data).T), \n",
    "                                        (np.asarray(train_label).T), \n",
    "                                        (np.asarray(test_data).T), \n",
    "                                        (np.asarray(test_label).T))\n",
    "    losses.append(loss)\n",
    "    print(f\"\\tloss: {loss}\")\n",
    "    print(\"\\ttime: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        \n",
    "    f.write(f\"fold = {i}\")\n",
    "    f.write(f\"\\tloss: {loss}\\n\")\n",
    "    f.write(\"\\ttime: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +\"\\n\")\n",
    "\n",
    "    torch.save(model.state_dict(), PATH+'model_'+str(i))\n",
    "f.write(\"train END: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62b1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 230516: takes 78 minutes per fold! loss is 412\n",
    "# After 700 minute, loss is:\n",
    "# we are on fold no.0\n",
    "# \tloss: 412.9017034505208\n",
    "# we are on fold no.1\n",
    "# \tloss: 412.9801847330729\n",
    "# we are on fold no.2\n",
    "# \tloss: 412.79992659505206\n",
    "# we are on fold no.3\n",
    "# \tloss: 412.94208658854166\n",
    "# we are on fold no.4\n",
    "# \tloss: 412.96778076171876\n",
    "# we are on fold no.5\n",
    "# \tloss: 412.6050344238281\n",
    "# we are on fold no.6\n",
    "# \tloss: 412.8362565917969\n",
    "# we are on fold no.7\n",
    "# \tloss: 413.03926790364585\n",
    "# we are on fold no.8\n",
    "# \tloss: 413.1065719401042\n",
    "# we are on fold no.9\n",
    "# \tloss: 413.0045514322917\n",
    "# intuition, label is about 20, error is MSE, \n",
    "# so my predictions are always 0 or 1-ish!\n",
    "# Problem: used sigmoid(y) at end of neural network!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c388d3df-3754-4852-b39c-497b69f65682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean losses: 0.22448036237061028, std: 0.04588106152693985\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean losses: {np.mean(losses)}, std: {np.std(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50809583",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we use 162th example\n",
      "\tthe prediction (normalized) is: -0.9298792481422424\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 180th example\n",
      "\tthe prediction (normalized) is: -2.0472285747528076\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 356th example\n",
      "\tthe prediction (normalized) is: -1.262682318687439\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 375th example\n",
      "\tthe prediction (normalized) is: -1.2654244899749756\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 427th example\n",
      "\tthe prediction (normalized) is: -1.097796082496643\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 501th example\n",
      "\tthe prediction (normalized) is: -1.434181571006775\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 558th example\n",
      "\tthe prediction (normalized) is: -1.610810399055481\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 566th example\n",
      "\tthe prediction (normalized) is: -0.6301397681236267\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 664th example\n",
      "\tthe prediction (normalized) is: -1.6403799057006836\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n",
      "we use 777th example\n",
      "\tthe prediction (normalized) is: -0.9990759491920471\n",
      "\tthe ground truth (normalized) is: -1.5849058664490416\n"
     ]
    }
   ],
   "source": [
    "number_figures = 10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indices = torch.randint(0,len(spectrum),(number_figures,)).unique()\n",
    "for i in indices:\n",
    "    print(f\"we use {i}th example\")\n",
    "    # change: cast i to int, since pandas not work with torch.int64\n",
    "    # changed: removed figure, since the output is just one number\n",
    "    spec = np.asarray(spectrum[int(i)]).flatten()\n",
    "    temp = np.asarray(temperature[int(i)]).flatten()\n",
    "    # plt.figure(i)\n",
    "\n",
    "    prediction = model(torch.Tensor(np.asarray(spectrum[int(i)])).to(model.device)).detach().cpu().flatten()\n",
    "    # plt.plot(prediction)\n",
    "    print(f\"\\tthe prediction (normalized) is: {prediction.item()}\")\n",
    "    # plt.plot(temp)\n",
    "    # plt.legend(['reconstruction','ground truth'])\n",
    "    print(f\"\\tthe ground truth (normalized) is: {temp.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "652a6d23-3f62-43cf-a619-ea946afe62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "I calculate coefficients like pearsonr with respect to all test examples and their labels\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "pearson\n",
      "0.8847269879241996 0.023893769074949393\n",
      "0.0 0.0\n",
      "kendall\n",
      "0.7493965290947531 0.030957607682910573\n",
      "0.0 0.0\n",
      "spearman\n",
      "0.8906862179539354 0.020438000456394396\n",
      "0.0 0.0\n",
      "Absolute Error\n",
      "5\n",
      "0.020276281057922042\n",
      "50\n",
      "0.2325447021345507\n",
      "90\n",
      "0.8002219724030599\n",
      "95\n",
      "1.0396681156649568\n",
      "99\n",
      "1.5282319440592955\n",
      "Distance Correlation\n",
      "0.884726990705244 0.023893778790746403\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = 'model_dnn/'\n",
    "device = torch.device(\"cuda\")\n",
    "mdl = Model(device=device, input_dim=1000)\n",
    "\n",
    "from scipy import stats,spatial\n",
    "#pip install dtw-python\n",
    "from dtw import * # <- this is dtw-python, NOT dtw package! (different dtw()s)\n",
    "import torch\n",
    "import numpy as np\n",
    "# response = pd.read_csv(\"1127_final_data/response.csv\", header=None).values #input X\n",
    "# spectra = pd.read_csv(\"1127_final_data/spectra.csv\", header=None).values #ground truth label Y\n",
    "correlation_losses = []\n",
    "\n",
    "print(\"I calculate coefficients like pearsonr with respect to all test examples and their labels\")\n",
    "def calculate_correlation(model, test_data, test_label):\n",
    "    test_data_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "    # print(f\"shape of test_data_tensor: {test_data_tensor.shape}\")\n",
    "    construction = model(test_data_tensor).detach().cpu().numpy()\n",
    "    # print(f\"shape of construction: {construction.shape}\")\n",
    "    # Pearson\n",
    "    pearson_coefs = []\n",
    "    pearson_ps = []\n",
    "    \n",
    "    # Kendall\n",
    "    kendall_coefs = []\n",
    "    kendall_ps = []\n",
    "    \n",
    "    # Spearman\n",
    "    spearman_coefs = []\n",
    "    spearman_ps = []\n",
    "    \n",
    "    # Distance Correlation\n",
    "    distance_corr = []\n",
    "    \n",
    "    #DTW distance\n",
    "    alignment = []\n",
    "    \n",
    "    #absolute_error\n",
    "    abs_err = []\n",
    "    \n",
    "    # print(f\"test_label.shape[0]: {test_label.shape[0]}\")\n",
    "    for i in range(test_label.shape[1]):\n",
    "        x1 = np.reshape(construction, -1)\n",
    "        # print(f\"shape of x1: {x1.shape}\")\n",
    "        x2 = np.reshape(test_label, -1)\n",
    "        # print(f\"shape of x2: {x2.shape}\")\n",
    "        res = stats.pearsonr(x1, x2)\n",
    "        pearson_coefs.append(res[0])\n",
    "        pearson_ps.append(res[1])\n",
    "        \n",
    "        res = stats.kendalltau(x1, x2)\n",
    "        kendall_coefs.append(res[0])\n",
    "        kendall_ps.append(res[1])\n",
    "        \n",
    "        res = stats.spearmanr(x1, x2)\n",
    "        spearman_coefs.append(res[0])\n",
    "        spearman_ps.append(res[1])\n",
    "        \n",
    "        distance_corr.append(1- spatial.distance.correlation(x1,x2))\n",
    "\n",
    "        # change: removed dtw <- this is for time series, \n",
    "        # time is not involved in this project (I think!)\n",
    "        # alignment.append(dtw(x1, x2, distance_only=True).distance)\n",
    "        abs_err.append(abs(x1-x2))\n",
    "        \n",
    "    correlation_results = {\n",
    "        'pearson': (pearson_coefs, pearson_ps),\n",
    "        'kendall': (kendall_coefs, kendall_ps),\n",
    "        'spearman': (spearman_coefs, spearman_ps),\n",
    "        # 'DTW': alignment,\n",
    "        'Absolute Error': abs_err,\n",
    "        'Distance Correlation': distance_corr\n",
    "    }\n",
    "\n",
    "    return correlation_results\n",
    "\n",
    "for i, (train, test) in enumerate(zip(train_indices, test_indices)):\n",
    "    print(i)\n",
    "    train_data, train_label = spectrum[train], temperature[train]\n",
    "    test_data, test_label = spectrum[test], temperature[test]\n",
    "    \n",
    "    mdl_name = PATH + 'model_' + str(i)\n",
    "    mdl.load_state_dict(torch.load(mdl_name))\n",
    "    mdl.eval()\n",
    "    \n",
    "    correlation_loss = calculate_correlation(mdl, \n",
    "                                             np.transpose(np.asarray(test_data)), \n",
    "                                             np.transpose(np.asarray(test_label)))\n",
    "    correlation_losses.append(correlation_loss)\n",
    "for key in correlation_losses[0].keys():\n",
    "    print(key)\n",
    "    if key=='Absolute Error':\n",
    "        errors = []\n",
    "        for d in correlation_losses:\n",
    "            errors+=np.concatenate(d[key]).ravel().tolist()\n",
    "        #percentile\n",
    "        percentiles = [5, 50, 90, 95, 99]\n",
    "        for p in percentiles:\n",
    "            print(p)\n",
    "            print(np.percentile(errors, p))\n",
    "    else:\n",
    "        stat, p = [], []\n",
    "        for d in correlation_losses:\n",
    "            if key=='DTW' or key=='Distance Correlation':\n",
    "                stat+=d[key]\n",
    "            else:\n",
    "                stat+=d[key][0]\n",
    "                p+=d[key][1]\n",
    "        print(np.mean(stat),np.std(stat))\n",
    "        if len(p)>0:\n",
    "            print(np.mean(p),np.std(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ccc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
