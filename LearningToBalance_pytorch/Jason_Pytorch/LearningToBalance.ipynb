{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"setup information\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib as imp\n",
    "# specific classes for training, validating, and testing, \n",
    "# as described in LearningToBalance original code\n",
    "train_class_names = [\n",
    "    'train', 'skyscraper', 'turtle', 'raccoon', 'spider', 'orange', 'castle', 'keyboard',\n",
    "    'clock', 'pear', 'girl', 'seal', 'elephant', 'apple', 'aquarium_fish', 'bus',\n",
    "    'mushroom', 'possum', 'squirrel', 'chair', 'tank', 'plate', 'wolf', 'road', 'mouse',\n",
    "    'boy', 'shrew', 'couch', 'sunflower', 'tiger', 'caterpillar', 'lion', 'streetcar',\n",
    "    'lawn_mower', 'tulip', 'forest', 'dolphin', 'cockroach', 'bear', 'porcupine', 'bee',\n",
    "    'hamster', 'lobster', 'bowl', 'can', 'bottle', 'trout', 'snake', 'bridge',\n",
    "    'pine_tree', 'skunk', 'lizard', 'cup', 'kangaroo', 'oak_tree', 'dinosaur', 'rabbit',\n",
    "    'orchid', 'willow_tree', 'ray', 'palm_tree', 'mountain', 'house', 'cloud'\n",
    "    ]\n",
    "valid_class_names = [\n",
    "    'otter', 'motorcycle', 'television', 'lamp', 'crocodile', 'shark', 'butterfly', 'sea',\n",
    "    'beaver', 'beetle', 'tractor', 'flatfish', 'maple_tree', 'camel', 'crab', 'cattle'\n",
    "    ]\n",
    "test_class_names = [\n",
    "    'baby', 'bed', 'bicycle', 'chimpanzee', 'fox', 'leopard', 'man', 'pickup_truck',\n",
    "    'plain', 'poppy', 'rocket', 'rose', 'snail', 'sweet_pepper', 'table', 'telephone',\n",
    "    'wardrobe', 'whale', 'woman', 'worm'\n",
    "    ]\n",
    "# \n",
    "LABEL_NAMES : list[str] = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/new/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model, get_parameter, generating params theta\n",
      "model, get_parameter, generating params alpha\n",
      "T-R-A-I-N\n",
      "train_dataloader length? 16\n",
      "valid_dataloader length? 2\n",
      "V-A-L-I-D\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 25.00 GiB already allocated; 10.75 MiB free; 25.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     16\u001b[0m m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mLearningToBalance(data_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcifar\u001b[39m\u001b[39m'\u001b[39m, number_of_inner_gradient_steps\u001b[39m=\u001b[39mnumber_of_inner_gradient_steps, \n\u001b[1;32m     17\u001b[0m         ways\u001b[39m=\u001b[39mways, shots\u001b[39m=\u001b[39mshots, inner_learning_rate\u001b[39m=\u001b[39minner_learning_rate, outer_learning_rate\u001b[39m=\u001b[39mouter_learning_rate, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[0;32m---> 18\u001b[0m train_loss, valid_loss, train_accuracy, valid_accuracy \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     19\u001b[0m         train_dataloader\u001b[39m=\u001b[39;49mData\u001b[39m.\u001b[39;49mget_dataloader(train_class_names, shots, query, ways, total_train_task, batch_size),\n\u001b[1;32m     20\u001b[0m         valid_dataloader\u001b[39m=\u001b[39;49mData\u001b[39m.\u001b[39;49mget_dataloader(valid_class_names, shots, query, ways, total_valid_task, batch_size))\n\u001b[1;32m     21\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mtest(test_dataloader\u001b[39m=\u001b[39m\n\u001b[1;32m     22\u001b[0m        Data\u001b[39m.\u001b[39mget_dataloader(test_class_names, shots, query, ways, total_test_task, batch_size))\n\u001b[1;32m     23\u001b[0m f, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m), nrows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ncols\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/ml/LearningToBalance_pytorch/Jason_Pytorch/model.py:314\u001b[0m, in \u001b[0;36mLearningToBalance.train\u001b[0;34m(self, train_dataloader, valid_dataloader)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m# print(f\"train step {step}\")\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m# enumerated dataloader is a list[tuple[stuffs]]\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# print(f\"type of dataloader: {type(train_dataloader)}\")\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# print(f\"type of task batch: {type(task_batch)}\")\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 314\u001b[0m outer_loss, outer_accuracy, KL, inner_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer_step_(task_batch, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    315\u001b[0m \u001b[39m# print(f\"ran _outer_step()\")\u001b[39;00m\n\u001b[1;32m    316\u001b[0m outer_loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ml/LearningToBalance_pytorch/Jason_Pytorch/model.py:278\u001b[0m, in \u001b[0;36mLearningToBalance._outer_step_\u001b[0;34m(self, input_task_batch, train)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m    274\u001b[0m \u001b[39m# \"\"\"possible improvement: use torch.vmap instead of calculating one by one\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m   \u001b[39m# print(f\"length of input_task_batch: {input_task[0].shape}\")\u001b[39;00m\n\u001b[1;32m    276\u001b[0m   input_task \u001b[39m=\u001b[39m (input_task_batch[\u001b[39m0\u001b[39m][t], input_task_batch[\u001b[39m1\u001b[39m][t], \n\u001b[1;32m    277\u001b[0m                 input_task_batch[\u001b[39m2\u001b[39m][t], input_task_batch[\u001b[39m3\u001b[39m][t])\n\u001b[0;32m--> 278\u001b[0m   cross_entropy, outer_accuracy, KL, prediction, inner_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer_step_single_task(input_task, train)\n\u001b[1;32m    279\u001b[0m   \u001b[39m# print(f\"cross_entropy {cross_entropy}\")\u001b[39;00m\n\u001b[1;32m    280\u001b[0m   cross_entropy_list\u001b[39m.\u001b[39mappend(cross_entropy)\n",
      "File \u001b[0;32m~/ml/LearningToBalance_pytorch/Jason_Pytorch/model.py:254\u001b[0m, in \u001b[0;36mLearningToBalance._outer_step_single_task\u001b[0;34m(self, input_task, train)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameter\u001b[39m.\u001b[39mupdate(theta_update_by_zeta)\n\u001b[1;32m    253\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"inner gradient steps; omega & gamma for task_specific modulation\"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m theta_update_by_inner_loop, inner_accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inner_loop(x_train, y_train, train, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameter, omega, gamma)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameter\u001b[39m.\u001b[39mupdate(theta_update_by_inner_loop)\n\u001b[1;32m    256\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"outer-loss & test_accuracy\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/LearningToBalance_pytorch/Jason_Pytorch/model.py:204\u001b[0m, in \u001b[0;36mLearningToBalance._inner_loop\u001b[0;34m(self, x_train, y_train, train, theta, omega, gamma)\u001b[0m\n\u001b[1;32m    200\u001b[0m inner_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39momega_modify_loss_of_class(omega, cross_entropy_per_class)\n\u001b[1;32m    201\u001b[0m \u001b[39m# make computation graph, so 2nd-order derivativa can be calculated\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# when we call .backward() on OUTER loss\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# when train, DO make graph, to allow back propagation; when validate, NO make graph!\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m grads \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49minner_loss, inputs\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(theta\u001b[39m.\u001b[39;49mvalues()), create_graph\u001b[39m=\u001b[39;49mtrain)\n\u001b[1;32m    205\u001b[0m gradient_dictionary : \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(theta\u001b[39m.\u001b[39mkeys(), grads))\n\u001b[1;32m    206\u001b[0m accuracy_list\u001b[39m.\u001b[39mappend(inner_accuracy)\n",
      "File \u001b[0;32m~/anaconda3/envs/new/lib/python3.11/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 25.00 GiB already allocated; 10.75 MiB free; 25.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import Data\n",
    "imp.reload(Data)\n",
    "imp.reload(model)\n",
    "# data_name : str = 'cifar'\n",
    "number_of_inner_gradient_steps: int = 1\n",
    "ways: int = 7\n",
    "shots: int = 10\n",
    "query: int = 3\n",
    "total_train_task = 80\n",
    "total_valid_task = 10\n",
    "total_test_task = 20\n",
    "inner_learning_rate: float = 1e-4\n",
    "outer_learning_rate: float = 1e-4\n",
    "batch_size = 5\n",
    "m = model.LearningToBalance(data_name='cifar', number_of_inner_gradient_steps=number_of_inner_gradient_steps, \n",
    "        ways=ways, shots=shots, inner_learning_rate=inner_learning_rate, outer_learning_rate=outer_learning_rate, batch_size=batch_size)\n",
    "train_loss, valid_loss, train_accuracy, valid_accuracy = m.train(\n",
    "        train_dataloader=Data.get_dataloader(train_class_names, shots, query, ways, total_train_task, batch_size),\n",
    "        valid_dataloader=Data.get_dataloader(valid_class_names, shots, query, ways, total_valid_task, batch_size))\n",
    "test_loss, test_accuracy = m.test(test_dataloader=\n",
    "       Data.get_dataloader(test_class_names, shots, query, ways, total_test_task, batch_size))\n",
    "f, ax = plt.subplots(figsize=(4, 4), nrows=1, ncols=3)\n",
    "ax[0].plot(train_loss)\n",
    "ax[1].plot(valid_loss)\n",
    "ax[2].plot(test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
