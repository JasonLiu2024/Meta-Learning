{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "s = {\n",
    "    'problem'           : \"regression\",\n",
    "    'approach'          : \"few-shot learning\",\n",
    "    'method'            : \"non-parametric\",\n",
    "    'algorithm'         : \"siamese network\",\n",
    "    'goal'              : \"learn a distribution using few samples from it\",\n",
    "    'input'             : \"samples from a distribution\",\n",
    "    'input type'        : \"vectors\",\n",
    "    'input meaning'     : \"spectrum\", \n",
    "    'output'            : \"samples from a distribution\",\n",
    "    'output type'       : \"one number\",\n",
    "    'output meaning'    : \"temperature or pressure, depending on distribution\",\n",
    "    'number of ways'    : 2,\n",
    "    'number of shots'   : 1,\n",
    "    'number of folds'   : 8,\n",
    "    'support-query ratio': 0.8,\n",
    "    'task size'         : 5,\n",
    "    'learning rate'     : 1e-4,\n",
    "    'input dimension'   : 10000,\n",
    "    'output dimension'  : 1,\n",
    "    'feature dimension' : 300,\n",
    "    'epoch'             : 1000,\n",
    "    'epoch development' : 100,\n",
    "    'data'              : 'temperature_230509_discrete',\n",
    "    'data P'            : 'pressure_230516_discrete',\n",
    "    'data T'            : 'temperature_230509_discrete',\n",
    "    'cross validation round': 16,\n",
    "    'cross validation round development' : 3,\n",
    "    'batch size'        : 32,\n",
    "    'best model folder' : 'single_T_best_model/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading temperature_230509_discrete__________________________\n",
      "\tinput shape (number, dimension): (6000, 10000)\n",
      "\tlabel shape (number, dimension): (6000, 1)\n",
      "\tthere are 16 folds\n",
      "\t4200 for training, 600 for validating, 1200 for testing\n",
      "loading pressure_230516_discrete__________________________\n",
      "\tinput shape (number, dimension): (5000, 10000)\n",
      "\tlabel shape (number, dimension): (5000, 1)\n",
      "\tthere are 16 folds\n",
      "\t3500 for training, 500 for validating, 1000 for testing\n"
     ]
    }
   ],
   "source": [
    "import data_accessor as acc\n",
    "data_names_list = [\n",
    "    'temperature_230509_discrete',\n",
    "    'pressure_230516_discrete'\n",
    "    ]\n",
    "data_dictionary = acc.setup(data_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SingleTaskNetwork(torch.nn.Module):\n",
    "    def __init__(self, device, input_dimension, feature_dimension, output_dimension):\n",
    "        \"\"\" Input: input, anchor, anchor label\n",
    "        Output: prediction for input\"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.hidden_dimension = 300\n",
    "        self.feature_hidden_dimension = 36\n",
    "        self.feature_dimension = feature_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.device = device\n",
    "        self.feature_sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_dimension, self.hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dimension, self.hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dimension, self.feature_dimension)\n",
    "        )\n",
    "        self.auxiliary_sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.feature_dimension, self.feature_hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(self.feature_hidden_dimension, self.feature_hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Linear(self.feature_hidden_dimension, self.output_dimension)\n",
    "        )\n",
    "        self.to(device)\n",
    "        self.float()\n",
    "    def forward(self, input):\n",
    "        feature_input = self.feature_sequential(input)\n",
    "        prediction = self.auxiliary_sequential(feature_input)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import SaveBestModel, PatienceEarlyStopping, Scheduler, plot_loss\n",
    "class Manager:\n",
    "    \"\"\" DOES: train & evaluate a Siamese network\n",
    "        \"\"\"\n",
    "    def __init__(self, epoch, cross_validation_round):\n",
    "        self._network = SingleTaskNetwork(device, s['input dimension'], s['feature dimension'], s['output dimension'])\n",
    "        self._network.apply(self.initializer)\n",
    "        self._learning_rate = s['learning rate']\n",
    "        self._optimizer = torch.optim.Adam(\n",
    "            params=self._network.parameters(), lr=self._learning_rate,\n",
    "            weight_decay=3e-3)\n",
    "        self._energy = nn.MSELoss()\n",
    "        self._train_loss = []\n",
    "        self._valid_loss = []\n",
    "        self._test_loss = []\n",
    "        self._epoch = epoch\n",
    "        self._stopper = PatienceEarlyStopping(patience=5, min_delta=1e-7)\n",
    "        self._cross_validation_round = cross_validation_round\n",
    "        self._saver = SaveBestModel(s['best model folder'])\n",
    "        self._scheduler = Scheduler(optimizer=self._optimizer, \n",
    "            minimum_learning_rate=1e-6, patience=5, factor=0.5)\n",
    "    def initializer(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(layer.weight) # normal version\n",
    "    def _step(self, job):\n",
    "        input, input_label = job\n",
    "        # print(f\"input dtype is {input_1.dtype}\")\n",
    "        prediction = self._network(input)\n",
    "        loss = self._energy(input_label, prediction)\n",
    "        return loss     \n",
    "    def train(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\" DOES: calculate loss from tasks\n",
    "            NOTE: we have a BATCH of tasks here \"\"\"\n",
    "        for e in range(self._epoch):\n",
    "            # print(f\"train() epoch {e}\")\n",
    "            batch_train_loss = []\n",
    "            for _, batch in enumerate(train_dataloader): \n",
    "                self._optimizer.zero_grad()\n",
    "                loss = self._step(batch)\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "                batch_train_loss.append(loss.item())\n",
    "            self._train_loss.append(np.mean(batch_train_loss))\n",
    "            batch_valid_loss = []\n",
    "            with torch.no_grad():\n",
    "                for _, batch in enumerate(valid_dataloader): \n",
    "                    loss = self._step(batch)\n",
    "                    batch_valid_loss.append(loss.item())\n",
    "            self._valid_loss.append(np.mean(batch_valid_loss))\n",
    "            # saving, early stopping, scheduler for EACH epoch!\n",
    "            self._saver(current_loss=np.mean(batch_valid_loss), \n",
    "                  model=self._network, \n",
    "                  round=self._cross_validation_round\n",
    "                  )\n",
    "            self._scheduler(np.mean(batch_valid_loss))\n",
    "            self._stopper(np.mean(batch_valid_loss))\n",
    "            if self._stopper.early_stop == True:\n",
    "                print(f\"EARLY STOPPING @ epoch {e}\")\n",
    "                break\n",
    "        # summary printout, after we're done with epochs\n",
    "        print(f\"min train loss: {np.min(self._train_loss)}\")\n",
    "        print(f\"min valid loss: {np.min(self._valid_loss)}\")\n",
    "        plot_loss(self._train_loss, self._valid_loss)\n",
    "        return np.min(self._valid_loss)\n",
    "    def test(self, test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch_test_loss = []\n",
    "            for _, batch in enumerate(test_dataloader): \n",
    "                loss = self._step(batch)\n",
    "                batch_test_loss.append(loss.item())\n",
    "            self._test_loss.append(np.mean(batch_test_loss)) \n",
    "        return np.min(self._test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tools import DefaultDataset, SaveBestCrossValidationModel\n",
    "\n",
    "CV_saver = SaveBestCrossValidationModel(s['best model folder'])\n",
    "test_indices = data_dictionary[s['data T']]['test indices']\n",
    "epoch = s['epoch']\n",
    "print(f\"data: {s['data P']} then {s['data T']}\")\n",
    "cross_validation_loss = []\n",
    "for cross_validation_round in range(s['cross validation round']):\n",
    "    if cross_validation_round < s['cross validation round']:\n",
    "        print(f\"CV round {cross_validation_round}_________________________________\")\n",
    "        network_object = Manager(epoch, cross_validation_round)\n",
    "        print(f\"using {s['data P']}\")\n",
    "        _ = network_object.train(\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data P']]['data'],\n",
    "            data_dictionary[s['data P']]['label'],\n",
    "            data_dictionary[s['data P']]['train indices'][cross_validation_round],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']),\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data P']]['data'],\n",
    "            data_dictionary[s['data P']]['label'],\n",
    "            data_dictionary[s['data P']]['valid indices'][cross_validation_round],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']))\n",
    "        print(f\"using {s['data T']}\")\n",
    "        network_object._saver.reset()\n",
    "        network_object._stopper.reset()\n",
    "        network_object._train_loss = []\n",
    "        network_object._valid_loss = []\n",
    "        print(f\"reset: train & valid loss, early stopper, saver\")\n",
    "        valid_loss = network_object.train(\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data T']]['data'],\n",
    "            data_dictionary[s['data T']]['label'],\n",
    "            data_dictionary[s['data T']]['train indices'][cross_validation_round],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']),\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data T']]['data'],\n",
    "            data_dictionary[s['data T']]['label'],\n",
    "            data_dictionary[s['data T']]['valid indices'][cross_validation_round],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']))\n",
    "        CV_saver(current_loss=valid_loss, round=cross_validation_round)\n",
    "        cross_validation_loss.append(valid_loss)\n",
    "print()\n",
    "print(f\"\\nbest model is: {CV_saver.best_model_name} with {CV_saver.current_best_loss}\")\n",
    "print(f\"The aggregate performance is: mean {np.mean(cross_validation_loss)}, std {np.std(cross_validation_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss: for temperature_230509_discrete: 280.7644894248561\n",
      "testing loss: for pressure_230516_discrete: 0.0003345543432260456\n"
     ]
    }
   ],
   "source": [
    "network_object._network.load_state_dict(torch.load(s['best model folder'] + CV_saver.best_model_name))\n",
    "test_loss = network_object.test(\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data P']]['data'],\n",
    "            data_dictionary[s['data P']]['label'],\n",
    "            data_dictionary[s['data P']]['test indices'],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']))\n",
    "print(f\"testing loss: for {s['data P']}: {test_loss}\")\n",
    "test_loss = network_object.test(\n",
    "            DataLoader(DefaultDataset(\n",
    "            data_dictionary[s['data T']]['data'],\n",
    "            data_dictionary[s['data T']]['label'],\n",
    "            data_dictionary[s['data T']]['test indices'],\n",
    "            device=device,), shuffle=False, batch_size=s['batch size']))\n",
    "print(f\"testing loss: for {s['data T']}: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
