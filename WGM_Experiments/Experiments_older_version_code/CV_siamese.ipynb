{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f724513",
   "metadata": {},
   "source": [
    "# **run load_data.ipynb BEFORE running this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d395e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "d = torch.distributions.Normal(0, 1)\n",
    "d.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = {}\n",
    "for l in [1,2,3,4]:\n",
    "        gamma['conv%d_w'%l] = gamma['conv%d_b'%l] = g_[l-1]\n",
    "        gamma['dense_w'] = gamma['dense_b'] = g_[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49acb3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv3_w'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 3\n",
    "'conv%d_w'%l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ca00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data from data.pickle\n",
      "shape of spectrum data: (6000, 10000)\n",
      "shape of temperature data: (6000, 1)\n",
      "\n",
      "there are 6000 spectrums\n",
      "each spectrum is 10000 long, which is number of features\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import numpy as np\n",
    "PATH = 'model_siamese/'\n",
    "data_file_name = 'data'\n",
    "\n",
    "with open(data_file_name + '.pickle', 'rb') as handle:\n",
    "    spectrum, temperature = pickle.load(handle)\n",
    "    print(f\"read data from {data_file_name}.pickle\")\n",
    "print(f\"shape of spectrum data: {spectrum.shape}\")\n",
    "print(f\"shape of temperature data: {temperature.shape}\")\n",
    "print()\n",
    "print(f\"there are {temperature.shape[0]} spectrums\")\n",
    "print(f\"each spectrum is {spectrum.shape[1]} long, which is number of features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8584e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got indices from indices.pickle\n",
      "\n",
      "sets of training indices: 16\n",
      "number of training indices per set: 4200\n",
      "sets of validating indices: 16\n",
      "number of validating indices per set: 600\n",
      "number of testing indices: 1200\n",
      "\n",
      "input dimension is:   10000\n",
      "number of samples is: 6000\n",
      "output dimension is:  1\n"
     ]
    }
   ],
   "source": [
    "indices_file_name = 'indices'\n",
    "with open(indices_file_name + '.pickle', 'rb') as handle:\n",
    "    train_indices, validate_indices, test_indices = pickle.load(handle)\n",
    "    print(f\"got indices from {indices_file_name}.pickle\")  \n",
    "print()\n",
    "print(f\"sets of training indices: {len(train_indices)}\")\n",
    "print(f\"number of training indices per set: {len(train_indices[0])}\")\n",
    "print(f\"sets of validating indices: {len(validate_indices)}\")\n",
    "print(f\"number of validating indices per set: {len(validate_indices[0])}\")\n",
    "print(f\"number of testing indices: {len(test_indices)}\")\n",
    "print()\n",
    "input_dimension = spectrum.shape[1]\n",
    "print(f\"input dimension is:   {input_dimension}\")\n",
    "number_of_samples = spectrum.shape[0]\n",
    "print(f\"number of samples is: {number_of_samples}\")\n",
    "output_dimension = temperature.shape[1]\n",
    "print(f\"output dimension is:  {output_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5b79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference:\n",
    "# https://arxiv.org/pdf/1607.02257.pdf Andreas Doumanoglou, \n",
    "#   Vassileios Balntas, Rigas Kouskouridas and Tae-Kyun Kim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0a47aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many parameters?\n",
    "#   \"a very simple two-layer ReLU network with p = 2n + d parameters \n",
    "#   that can express any labeling of any sample of size n in d dimensions\n",
    "#   https://arxiv.org/pdf/1611.03530.pdf\n",
    "#   https://stats.stackexchange.com/questions/320383/relationship-between-\n",
    "#   model-over-fitting-and-number-of-parameters/320387#320387\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "feature_dimension = 100\n",
    "class SiameseModel(torch.nn.Module):\n",
    "    def __init__(self, device, input_dim=input_dimension):\n",
    "        super().__init__()\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.hidden_dim = 32\n",
    "        self.feature_dim = feature_dimension\n",
    "        self.output_dim = output_dimension\n",
    "        # siamese part\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, self.hidden_dim),\n",
    "            self.relu,\n",
    "            # torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            self.relu,\n",
    "            # torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(self.hidden_dim, self.feature_dim)\n",
    "        )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        self.float()\n",
    "    def forward(self, input):\n",
    "        feature = self.sequential(input)\n",
    "        return feature\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self, device, input_dim=feature_dimension):\n",
    "        super().__init__()\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.feature_dim = feature_dimension\n",
    "        self.output_dim = output_dimension\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, self.feature_dim),\n",
    "            self.relu,\n",
    "            torch.nn.Linear(self.feature_dim, self.output_dim)\n",
    "        )\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        self.float()\n",
    "    def forward(self, feature):\n",
    "        output = self.sequential(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f01b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scheduler\n",
    "# https://arxiv.org/abs/1711.05101\n",
    "# Adam can substantially benefit from a scheduled learning rate multiplier. \n",
    "# The fact that Adam is an adaptive gradient algorithm and as such adapts \n",
    "# the learning rate for each parameter does not rule out the possibility to \n",
    "# substantially improve its performance by using a global learning rate \n",
    "# multiplier, scheduled, e.g., by cosine annealing.\n",
    "class Scheduler():\n",
    "    def __init__(self, optimizer, patience, minimum_learning_rate, factor):\n",
    "        # wait 'patience' number of epochs to change learning rate\n",
    "        # learning rates' lower bound: 'minimum_learning_rate'\n",
    "        # update learning rate by 'factor'ArithmeticError\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.minimum_learning_rate = minimum_learning_rate\n",
    "        self.factor = factor\n",
    "        # use 'min' mode because:\n",
    "        # we are monitoring loss\n",
    "        # we do stuff when loss stops DEcreasing\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', patience=self.patience,\n",
    "            factor=self.factor, min_lr=self.minimum_learning_rate,\n",
    "        )\n",
    "        # print(f\"SCHEDULER: {self.scheduler}:\")\n",
    "        # print(f\"\\tpatience = {self.patience}, factor = {self.factor}\" + \n",
    "        #       f\"minimum_learning_rate = {minimum_learning_rate}\")\n",
    "    def __call__(self, validation_loss):\n",
    "        self.scheduler.step(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee4cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "# https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping\n",
    "# -with-pytorch/\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience, min_delta):\n",
    "        # if no improvement after 'patience' epochs, stop training\n",
    "        # to count as improvement, need to change by 'min_delta' amount\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = loss\n",
    "        elif self.best_loss - loss >= self.min_delta:\n",
    "            # improved enough\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - loss < self.min_delta:\n",
    "            # did NOT improve enough :C\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                # it's stopping time! :C\n",
    "                # no need reset early_stop, because we only use it once\n",
    "                self.early_stop = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47111118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel():\n",
    "    def __init__(self, current_best_loss = float('inf')):\n",
    "        self.current_best_loss = current_best_loss\n",
    "        self.itsSavingTime = False\n",
    "    def __call__(self, current_loss, model_1, model_2, round):\n",
    "        # no save optimizer, since we are using model for inference\n",
    "        if current_loss < self.current_best_loss:\n",
    "            self.current_best_loss = current_loss\n",
    "            self.itsSavingTime = True\n",
    "        if self.itsSavingTime == True:\n",
    "            torch.save(model_1.state_dict(), PATH + f'fea_round={round}' + '.pth')\n",
    "            torch.save(model_2.state_dict(), PATH + f'reg_round={round}' + '.pth')\n",
    "# inherits from SaveBestModel\n",
    "class SaveBestModel_filename(SaveBestModel):\n",
    "    def __init__(self, current_best_loss = float('inf')):\n",
    "        SaveBestModel.__init__(self, current_best_loss = float('inf'))\n",
    "    def __call__(self, current_loss, round):\n",
    "        # no save optimizer, since we are using model for inference\n",
    "        if current_loss < self.current_best_loss:\n",
    "            self.current_best_loss = current_loss\n",
    "            self.current_best_model_1_filename = PATH + f'fea_round={round}' + '.pth'\n",
    "            self.current_best_model_2_filename = PATH + f'reg_round={round}' + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "577cc22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.22474487],\n",
       "       [ 0.        ],\n",
       "       [ 1.22474487]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StandardScaler example\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "arr = np.asarray([1, 2, 3]).reshape(-1, 1)\n",
    "sc = StandardScaler()\n",
    "arr = sc.fit_transform(arr)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee642ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "# reference:\n",
    "# https://colab.research.google.com/github/maticvl/dataHacker/\n",
    "# blob/master/pyTorch/014_siameseNetwork.ipynb#scrollTo=gD1BFFm_z7aj\n",
    "class SiameseDataset(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X is already normalized\n",
    "        self.y = y\n",
    "        self.y_normalize = torch.Tensor(\n",
    "            preprocessing.normalize(np.copy(y), axis=0)).to(device)\n",
    "        self.X = torch.Tensor(\n",
    "            preprocessing.normalize(np.copy(X), axis=0)).to(device)\n",
    "        self.indices = range(len(y))\n",
    "    # length: number of elements in subset of X_IDs\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    # get item by index in [m, n]\n",
    "    def __getitem__(self, index): # assume index lies within subset_X_IDs\n",
    "        # property: randomly sample two inputs,\n",
    "        # return: the two inputs, \n",
    "        #   and binary label for whethery they have same class\n",
    "        input_1_id = index\n",
    "        input_1 = self.X[input_1_id]\n",
    "        label_1 = self.y[input_1_id]\n",
    "        label_1_normalize = self.y_normalize[input_1_id]\n",
    "        input_2_id = random.choice(self.indices)\n",
    "        input_2 = self.X[input_2_id]\n",
    "        label_2 = self.y[input_2_id]\n",
    "        label_2_normalize = self.y_normalize[input_2_id]\n",
    "        return input_1, input_2, label_1, label_2, label_1_normalize, label_2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e28a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.24253563, 0.37139068, 0.4472136 ],\n",
       "       [0.9701425 , 0.92847669, 0.89442719]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize example\n",
    "from sklearn import preprocessing\n",
    "t = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "preprocessing.normalize(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3cc84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class FeatureLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureLoss, self).__init__()\n",
    "\n",
    "    def forward(self, feature_1, feature_2, label_1, label_2):\n",
    "        mse = nn.MSELoss()\n",
    "        feature_distance = mse(feature_1, feature_2)\n",
    "        # print(feature_distance)\n",
    "        label_distance = mse(label_1, label_2)\n",
    "        # print(label_distance)\n",
    "        return abs(feature_distance - label_distance)\n",
    "    \n",
    "# t_ = FeatureLoss()\n",
    "# l = t_(a, b, torch.Tensor([[0, 0], [1, 1]]), torch.Tensor([[0, 0], [1, 1]]))\n",
    "# l.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17847f0f-c9ae-49a7-a84d-e6f82f5cef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "# return: train_loss, validation_loss, current model\n",
    "class trainer():\n",
    "    def __init__(self, feature_model, regress_model, n_epochs, \n",
    "                 batch_size, \n",
    "                 feature_learning_rate,\n",
    "                 regress_learning_rate,\n",
    "                 cross_validation_round):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.cross_validation_round = cross_validation_round\n",
    "        self.device = device\n",
    "\n",
    "        self.feature_learning_rate = feature_learning_rate\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_model.apply(self.initializer)\n",
    "        self.feature_optimizer = optim.AdamW(self.feature_model.parameters(), \n",
    "                        lr=feature_learning_rate, weight_decay=3e-3) \n",
    "        self.feature_scheduler = Scheduler(optimizer=self.feature_optimizer, \n",
    "                        minimum_learning_rate=1e-6,\n",
    "                        patience=5,\n",
    "                        factor=0.5)\n",
    "        self.feature_criterion = FeatureLoss()\n",
    "        \n",
    "        self.regress_learning_rate = regress_learning_rate\n",
    "        self.regress_model = regress_model\n",
    "        self.regress_model.apply(self.initializer)\n",
    "        self.regress_optimizer = optim.AdamW(self.regress_model.parameters(), \n",
    "                        lr=regress_learning_rate, weight_decay=3e-3) \n",
    "        self.regress_scheduler = Scheduler(optimizer=self.regress_optimizer, \n",
    "                        minimum_learning_rate=1e-6,\n",
    "                        patience=5,\n",
    "                        factor=0.5)\n",
    "        self.regress_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.stopper = EarlyStopping(patience=5, min_delta=1e-6)\n",
    "    # use He initialization (because of RELU units in model definition)  \n",
    "    def initializer(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(layer.weight) # normal version\n",
    "    def get_loss(self, train_data, train_label, test_data, test_label):\n",
    "        # datasets\n",
    "        train_set = SiameseDataset(train_data, train_label)\n",
    "        val_set = SiameseDataset(train_data, test_label)\n",
    "        # dataloaders\n",
    "        loader_args = dict(batch_size=self.batch_size)\n",
    "        train_loader = DataLoader(train_set, shuffle=True, drop_last=True, \n",
    "                                  **loader_args)\n",
    "        val_loader = DataLoader(val_set, shuffle=True, drop_last=True, \n",
    "                                **loader_args)\n",
    "        \"\"\"SETUP\"\"\"\n",
    "        feature_train_loss = []\n",
    "        feature_valid_loss = []\n",
    "        regress_train_loss = []\n",
    "        regress_valid_loss = []\n",
    "\n",
    "        # scheduler action affects the optimizer!\n",
    "        saver = SaveBestModel()\n",
    "\n",
    "        for epoch in range(0, self.n_epochs):\n",
    "            # training\n",
    "            feature_train_loss_thisepoch = []\n",
    "            regress_train_loss_thisepoch = []\n",
    "            \n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                x1, x2, y1, y2, y1_normal, y2_normal = data\n",
    "                # print(f\"train: shape of x1: {x1.shape}\")\n",
    "                # get features\n",
    "                f1_pred = self.feature_model(x1.to(self.device))\n",
    "                f2_pred = self.feature_model(x2.to(self.device))\n",
    "                # get regressions\n",
    "                y1_pred = self.regress_model(f1_pred.to(self.device))\n",
    "                regress_loss_1 = self.regress_criterion(\n",
    "                    y1_pred.to(self.device).float(), y1.to(self.device).float())\n",
    "                self.regress_optimizer.zero_grad()\n",
    "                # print(f\"regress_loss_1's datatype: {regress_loss_1.dtype}\")\n",
    "                regress_loss_1.backward(retain_graph=True)\n",
    "                self.regress_optimizer.step()\n",
    "                regress_train_loss_thisepoch.append(regress_loss_1.item())\n",
    "\n",
    "                y2_pred = self.regress_model(f2_pred.to(self.device))\n",
    "                regress_loss_2 = self.regress_criterion(\n",
    "                    y2_pred.to(self.device).float(), y2.to(self.device).float())\n",
    "                self.regress_optimizer.zero_grad()\n",
    "                regress_loss_2.backward(retain_graph=True)\n",
    "                self.regress_optimizer.step()\n",
    "                regress_train_loss_thisepoch.append(regress_loss_2.item())\n",
    "\n",
    "                feature_loss = self.feature_criterion(\n",
    "                    f1_pred.to(self.device).float(), f2_pred.to(self.device).float(), \n",
    "                    y1_normal.to(self.device).float(), y2_normal.to(self.device).float())\n",
    "                self.feature_optimizer.zero_grad()\n",
    "                feature_loss.backward()\n",
    "                self.feature_optimizer.step()\n",
    "                feature_train_loss_thisepoch.append(feature_loss.item())\n",
    "            feature_train_loss.append(np.mean(feature_train_loss_thisepoch))\n",
    "            regress_train_loss.append(np.mean(regress_train_loss_thisepoch))\n",
    "\n",
    "            # validation\n",
    "            feature_valid_loss_thisepoch = []\n",
    "            regress_valid_loss_thisepoch = []\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                with torch.no_grad():\n",
    "                    x1, x2, y1, y2, y1_normal, y2_normal = data\n",
    "                    # print(f\"validate: shape of x1: {x1.shape}\")\n",
    "                    f1_pred = self.feature_model(x1.to(self.device))\n",
    "                    f2_pred = self.feature_model(x2.to(self.device))\n",
    "\n",
    "                    y1_pred = self.regress_model(f1_pred.to(self.device))\n",
    "                    regress_loss_1 = self.regress_criterion(\n",
    "                        y1_pred.to(self.device), y1.to(self.device))\n",
    "                    regress_valid_loss_thisepoch.append(regress_loss_1.item())\n",
    "\n",
    "                    y2_pred = self.regress_model(f2_pred.to(self.device))\n",
    "                    regress_loss_2 = self.regress_criterion(\n",
    "                        y2_pred.to(self.device), y2.to(self.device))\n",
    "                    regress_valid_loss_thisepoch.append(regress_loss_2.item())\n",
    "\n",
    "                    feature_loss = self.feature_criterion(\n",
    "                        f1_pred.to(self.device), f2_pred.to(self.device), \n",
    "                        y1_normal.to(self.device), y2_normal.to(self.device))\n",
    "                    feature_valid_loss_thisepoch.append(feature_loss.item())\n",
    "            feature_valid_loss.append(np.mean(feature_valid_loss_thisepoch))\n",
    "            regress_valid_loss.append(np.mean(regress_valid_loss_thisepoch))\n",
    "            # save best model\n",
    "            saver(current_loss=np.mean(feature_valid_loss_thisepoch), \n",
    "                  model_1=self.feature_model, \n",
    "                  model_2=self.regress_model,\n",
    "                  round=self.cross_validation_round\n",
    "                  )\n",
    "            # scheduler & early stopping\n",
    "            self.feature_scheduler(np.mean(feature_valid_loss_thisepoch))\n",
    "            self.regress_scheduler(np.mean(regress_valid_loss_thisepoch))\n",
    "\n",
    "            self.stopper(np.mean(regress_valid_loss_thisepoch))\n",
    "            if self.stopper.early_stop == True:\n",
    "                print(f\"\\tEARLY STOPPING @ epoch {epoch}\")\n",
    "                break\n",
    "        # plot result w.r.t epoch\n",
    "        #plot feature\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(feature_train_loss)\n",
    "        plt.plot(feature_valid_loss)\n",
    "        plt.title(\"FEATURE\")\n",
    "        #plot regression\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(regress_train_loss)\n",
    "        plt.plot(regress_valid_loss)\n",
    "        plt.title(\"REGRESS\")\n",
    "        plt.show()\n",
    "        min_feature_valid_loss = np.min(feature_valid_loss)\n",
    "        min_feature_train_loss = np.min(feature_train_loss)\n",
    "        min_regress_valid_loss = np.min(regress_valid_loss)\n",
    "        min_regress_train_loss = np.min(regress_train_loss)\n",
    "        return min_feature_valid_loss, min_feature_train_loss, self.feature_model, min_regress_valid_loss, min_regress_train_loss, self.regress_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6361355-4b60-4910-b158-c54bd4016da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's training time\n",
      "necessary params count: 22000\n",
      "actual params count: 324388\n",
      "\ttotal trainable parameters: 324388\n",
      "number of epochs: 2000, batch size: 64, device: cuda, learning rates 5e-05,5e-05\n",
      "NOTE: training loss is blue, validation loss is orange\n",
      "\n",
      "training starts:\n",
      "cross-validation round no.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcross-validation round no.\u001b[39m\u001b[39m{\u001b[39;00mcross_validation_round\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39m# use indices to extract training & testing set for current \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# cross-validation round\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_data, train_label \u001b[39m=\u001b[39m spectrum[train], temperature[train]\n\u001b[1;32m     53\u001b[0m validate_data, validate_label \u001b[39m=\u001b[39m spectrum[validate], temperature[validate]\n\u001b[1;32m     54\u001b[0m trainer_object \u001b[39m=\u001b[39m trainer(feature_model\u001b[39m=\u001b[39mfeature_mdl,\n\u001b[1;32m     55\u001b[0m                          regress_model\u001b[39m=\u001b[39mregress_mdl,\n\u001b[1;32m     56\u001b[0m                          n_epochs\u001b[39m=\u001b[39mn_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m                          regress_learning_rate\u001b[39m=\u001b[39mregress_learning_rate,\n\u001b[1;32m     60\u001b[0m                          cross_validation_round\u001b[39m=\u001b[39mcross_validation_round)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Theory suggests that when multiplying the batch size by k, \n",
    "# one should multiply the learning rate by sqrt(k) to keep the variance \n",
    "# in the gradient expectation constant. See page 5 at A. Krizhevsky. \n",
    "# One weird trick for parallelizing convolutional neural networks: \n",
    "# https://arxiv.org/abs/1404.5997\n",
    "# \n",
    "# However, recent experiments with large mini-batches suggest for a simpler \n",
    "# linear scaling rule, i.e multiply your learning rate by k when using \n",
    "# mini-batch size of kN. See P.Goyal et al.: Accurate, Large Minibatch SGD: \n",
    "# Training ImageNet in 1 Hour \n",
    "# https://arxiv.org/abs/1706.02677\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "print(\"it's training time\")\n",
    "\n",
    "n_epochs = 2000\n",
    "batch_size = 64\n",
    "feature_learning_rate = 5e-5\n",
    "regress_learning_rate = 5e-5\n",
    "\n",
    "\"\"\"model setup\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_mdl = SiameseModel(device=device, input_dim=input_dimension)\n",
    "regress_mdl = RegressionModel(device=device, input_dim=feature_dimension)\n",
    "\n",
    "print(f\"necessary params count: {2 * number_of_samples + input_dimension}\")\n",
    "total_params = sum(p.numel() for p in feature_mdl.parameters())\n",
    "print(f\"actual params count: {total_params}\")\n",
    "total_params_trainable = sum(p.numel() for p in feature_mdl.parameters() \n",
    "                             if p.requires_grad)\n",
    "print(f\"\\ttotal trainable parameters: {total_params_trainable}\")\n",
    "\n",
    "feature_train_losses = []\n",
    "feature_valid_losses = []\n",
    "regress_train_losses = []\n",
    "regress_valid_losses = []\n",
    "print(f\"number of epochs: {n_epochs}, batch size: {batch_size}, \" + \n",
    "      f\"device: {feature_mdl.device}, learning rates {feature_learning_rate},\" +\n",
    "      f\"{regress_learning_rate}\")\n",
    "print(\"NOTE: training loss is blue, validation loss is orange\")\n",
    "print()\n",
    "print(\"training starts:\")\n",
    "\n",
    "Path(PATH).mkdir(parents=True, exist_ok=True)\n",
    "saver = SaveBestModel_filename()\n",
    "for cross_validation_round, (train, validate) in enumerate(zip(train_indices, validate_indices)):\n",
    "    print(f\"cross-validation round no.{cross_validation_round}\")\n",
    "    # use indices to extract training & testing set for current \n",
    "    # cross-validation round\n",
    "    train_data, train_label = spectrum[train], temperature[train]\n",
    "    validate_data, validate_label = spectrum[validate], temperature[validate]\n",
    "    trainer_object = trainer(feature_model=feature_mdl,\n",
    "                             regress_model=regress_mdl,\n",
    "                             n_epochs=n_epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             feature_learning_rate=feature_learning_rate,\n",
    "                             regress_learning_rate=regress_learning_rate,\n",
    "                             cross_validation_round=cross_validation_round)\n",
    "    feature_valid_loss, feature_train_loss, feature_model, regress_valid_loss, regress_train_loss, regress_model = trainer_object.get_loss(\n",
    "                                        train_data, \n",
    "                                        train_label, \n",
    "                                        validate_data, \n",
    "                                        validate_label)\n",
    "    feature_train_losses.append(feature_train_loss)\n",
    "    feature_valid_losses.append(feature_valid_loss)\n",
    "    regress_train_losses.append(regress_train_loss)\n",
    "    regress_valid_losses.append(regress_valid_loss)\n",
    "\n",
    "    saver(current_loss=feature_valid_loss + regress_valid_loss, round=cross_validation_round)\n",
    "    print(f\"\\tminimum feature train loss:      {feature_train_loss}\")\n",
    "    print(f\"\\tminimum feature valid loss:      {feature_valid_loss}\")\n",
    "    print(f\"\\tminimum regress train loss:      {regress_train_loss}\")\n",
    "    print(f\"\\tminimum regress valid loss:      {regress_valid_loss}\")\n",
    "    print(\"\\ttime: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    break\n",
    "print()\n",
    "print(\"training and validation loss, across cross-validation rounds\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(feature_train_losses)\n",
    "plt.plot(feature_valid_losses)\n",
    "plt.title(\"FEATURE (CV)\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regress_train_losses)\n",
    "plt.plot(regress_valid_losses)\n",
    "plt.title(\"REGRESS (CV)\")\n",
    "plt.show()\n",
    "\n",
    "best_feature_model_filename = saver.current_best_model_1_filename\n",
    "best_regress_model_filename = saver.current_best_model_2_filename\n",
    "print(f\"the best feature model is:    {best_feature_model_filename}\")\n",
    "print(f\"the best regression model is: {best_regress_model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c388d3df-3754-4852-b39c-497b69f65682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature min validation losses: -1.1634897671481465e-06\n",
      "\tstd: 1.656300540248002e-07\n",
      "regress min validation losses: 0.12783715521997024\n",
      "\tstd: 0.016129173341523938\n",
      "\n",
      "loading best model\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "\n",
      "it's inference time!\n",
      "feature mean test loss: -4.72458973869531e-07\n",
      "regress mean test loss: 0.15599221872051275\n"
     ]
    }
   ],
   "source": [
    "print(f\"feature min validation losses: {np.min(feature_valid_losses)}\") \n",
    "print(f\"\\tstd: {np.std(feature_valid_losses)}\")\n",
    "print(f\"regress min validation losses: {np.min(regress_valid_losses)}\") \n",
    "print(f\"\\tstd: {np.std(regress_valid_losses)}\")\n",
    "print()\n",
    "\n",
    "print(\"loading best model\")\n",
    "best_feature_model = SiameseModel(device=device, input_dim=input_dimension)\n",
    "print(best_feature_model.load_state_dict(torch.load(best_feature_model_filename)))\n",
    "best_regress_model = RegressionModel(device=device, input_dim=feature_dimension)\n",
    "print(best_regress_model.load_state_dict(torch.load(best_regress_model_filename)))\n",
    "print()\n",
    "\n",
    "print(\"it's inference time!\")\n",
    "test = test_indices\n",
    "test_data, test_label = spectrum[test], temperature[test]\n",
    "test_set = SiameseDataset(\n",
    "            test_data, \n",
    "            test_label)\n",
    "loader_args = dict(batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=True, drop_last=True, \n",
    "                            **loader_args)\n",
    "feature_test_losses = []\n",
    "regress_test_losses = []\n",
    "feature_test_criterion = FeatureLoss()\n",
    "regress_test_criterion = nn.MSELoss()\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    with torch.no_grad():\n",
    "        x1, x2, y1, y2, y1_normal, y2_normal = data\n",
    "        # print(f\"validate: shape of x1: {x1.shape}\")\n",
    "        f1_pred = best_feature_model(x1.to(device))\n",
    "        f2_pred = best_feature_model(x2.to(device))\n",
    "\n",
    "        y1_pred = best_regress_model(f1_pred.to(device))\n",
    "        regress_loss_1 = regress_test_criterion(\n",
    "            y1_pred.to(device), y1.to(device))\n",
    "        regress_test_losses.append(regress_loss_1.item())\n",
    "\n",
    "        y2_pred = best_regress_model(f2_pred.to(device))\n",
    "        regress_loss_2 = regress_test_criterion(\n",
    "            y2_pred.to(device), y2.to(device))\n",
    "        regress_valid_losses.append(regress_loss_2.item())\n",
    "\n",
    "        feature_loss = feature_test_criterion(\n",
    "            f1_pred.to(device), f2_pred.to(device), \n",
    "            y1_normal.to(device), y2_normal.to(device))\n",
    "        feature_test_losses.append(feature_loss.item())\n",
    "print(f\"feature mean test loss: {np.mean(feature_test_losses)}\")\n",
    "print(f\"regress mean test loss: {np.mean(regress_test_losses)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50809583",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we use 40th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.32419204711914\n",
      "\tthe ground truth is: 21.5\n",
      "\tthe difference is: -0.17580795288085938\n",
      "we use 1548th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.322723388671875\n",
      "\tthe ground truth is: 21.5\n",
      "\tthe difference is: -0.177276611328125\n",
      "we use 1668th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.321609497070312\n",
      "\tthe ground truth is: 21.7\n",
      "\tthe difference is: -0.3783905029296868\n",
      "we use 3298th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.319686889648438\n",
      "\tthe ground truth is: 21.7\n",
      "\tthe difference is: -0.3803131103515618\n",
      "we use 3492th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.325956344604492\n",
      "\tthe ground truth is: 21.8\n",
      "\tthe difference is: -0.4740436553955085\n",
      "we use 4832th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.323257446289062\n",
      "\tthe ground truth is: 20.7\n",
      "\tthe difference is: 0.6232574462890632\n",
      "we use 5050th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.319568634033203\n",
      "\tthe ground truth is: 21.7\n",
      "\tthe difference is: -0.38043136596679616\n",
      "we use 5432th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.325153350830078\n",
      "\tthe ground truth is: 21.7\n",
      "\tthe difference is: -0.37484664916992116\n",
      "we use 5653th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.324562072753906\n",
      "\tthe ground truth is: 21.2\n",
      "\tthe difference is: 0.12456207275390696\n",
      "we use 5933th example\n",
      "-----------------------------------------------------\n",
      "\tthe prediction is: 21.32465934753418\n",
      "\tthe ground truth is: 21.0\n",
      "\tthe difference is: 0.3246593475341797\n"
     ]
    }
   ],
   "source": [
    "number_figures = 10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spec_normalize = preprocessing.normalize(np.copy(spectrum), axis=0)\n",
    "\n",
    "indices = torch.randint(0, len(spectrum),(number_figures,)).unique()\n",
    "for i in indices:\n",
    "    print(f\"we use {i}th example\")\n",
    "    # change: cast i to int, since pandas not work with torch.int64\n",
    "    # changed: removed figure, since the output is just one number\n",
    "    spec = np.asarray(spec_normalize[int(i)]).flatten()\n",
    "    temp = np.asarray(temperature[int(i)]).flatten()\n",
    "    \n",
    "    # print(np.asarray(spectrum[int(i)]).flatten())\n",
    "    # print(spec)\n",
    "    feature = best_feature_model(torch.Tensor(spec).to(device))\n",
    "    prediction = best_regress_model(feature).detach().cpu().flatten()\n",
    "    \n",
    "    prediction = prediction.item()\n",
    "    ground_truth = temp.item()\n",
    "    # recover data from normalization\n",
    "    print(f\"-----------------------------------------------------\")\n",
    "    print(f\"\\tthe prediction is: {prediction}\")\n",
    "    print(f\"\\tthe ground truth is: {ground_truth}\")\n",
    "    print(f\"\\tthe difference is: {prediction - ground_truth}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f48ad5",
   "metadata": {},
   "source": [
    "# **skip cell 14 for now (it's in template file!)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
